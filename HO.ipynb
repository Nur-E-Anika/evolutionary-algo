{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nur-E-Anika/evolutionary-algo/blob/main/HO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt-8eVeu2026",
        "outputId": "2ceabe5b-13d5-43f5-940a-610f086b9c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnnyNrHuywgq",
        "outputId": "e299d080-1149-4fb0-aa1a-41f1d465a69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGU7-d-J52Ux",
        "outputId": "452cef1a-b6a1-49ce-993d-9dd108dc6491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: nureanikaanan\n",
            "Your Kaggle Key: ··········\n",
            "Downloading titanic.zip to ./titanic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34.1k/34.1k [00:00<00:00, 46.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting archive ./titanic/titanic.zip to ./titanic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\"https://www.kaggle.com/competitions/titanic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IZAMU2fU6ipI"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy==1.24.4\n",
        "# !pip install pandas --upgrade\n",
        "# !pip install matplotlib --upgrade\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn --upgrade\n",
        "!pip install sklearn --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1axHCv4t7uwc",
        "outputId": "c109fa08-580f-444d-f99e-747500b35c9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
            "Installing collected packages: seaborn\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.12.2\n",
            "    Uninstalling seaborn-0.12.2:\n",
            "      Successfully uninstalled seaborn-0.12.2\n",
            "Successfully installed seaborn-0.13.0\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post9.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post9-py3-none-any.whl size=2952 sha256=9ebd1f1a6edbf4c2c5e5d713160e17d131b1a61dabcaf52b819b2cf9f6b216a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/a3/d2/092b519e9522b4c91608b7dcec0dd9051fa1bff4c45f4502d1\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1tPjiFCO6WmA"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oIXDK5nw8thp"
      },
      "outputs": [],
      "source": [
        "# load dataset using pandas\n",
        "titanic_train_df = pd.read_csv('./titanic/train.csv')\n",
        "titanic_test_df = pd.read_csv('./titanic/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2VDxi6DL9Ipn"
      },
      "outputs": [],
      "source": [
        "titanic_Y_col = titanic_train_df.columns[1]\n",
        "titanic_X_col = titanic_train_df.columns[2:]\n",
        "titanic_X_col = titanic_X_col.drop(['Name','Ticket'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DyPiK5Cn9Zqn"
      },
      "outputs": [],
      "source": [
        "titanic_X, titanic_Y = titanic_train_df[titanic_X_col].copy(), titanic_train_df[titanic_Y_col].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tJkd_rj99lsX"
      },
      "outputs": [],
      "source": [
        "numeric_cols = titanic_train_df[titanic_X_col].select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = titanic_train_df[titanic_X_col].select_dtypes(exclude=np.number).columns.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U_1TeKN79ryv"
      },
      "outputs": [],
      "source": [
        "# Impute and scale numeric columns\n",
        "imputer = SimpleImputer().fit(titanic_train_df[numeric_cols])\n",
        "titanic_X[numeric_cols] = imputer.transform(titanic_X[numeric_cols])\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler().fit(titanic_X[numeric_cols])\n",
        "titanic_X[numeric_cols] = scaler.transform(titanic_X[numeric_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5-fQpSv9yLI",
        "outputId": "68f0b546-99ab-4b76-8845-c204939676e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n"
          ]
        }
      ],
      "source": [
        "# One-hot encode categorical columns\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(titanic_X[categorical_cols])\n",
        "encoded_cols = list(encoder.get_feature_names_out(categorical_cols))\n",
        "titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "661JrU5T-owI"
      },
      "outputs": [],
      "source": [
        "titanic_X = titanic_X[numeric_cols + encoded_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B9NEtRtI-t-A"
      },
      "outputs": [],
      "source": [
        "titanic_X_Train, titanic_X_Test, titanic_Y_Train, titanic_Y_Test = train_test_split(titanic_X, titanic_Y, test_size = 0.30,random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p8pIPoNK-5dQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e179f1-5b56-41bd-b89a-3a24014e225a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymoo\n",
            "  Downloading pymoo-0.6.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from pymoo) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pymoo) (1.11.2)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from pymoo) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.10/dist-packages (from pymoo) (1.6.2)\n",
            "Collecting cma==3.2.2 (from pymoo)\n",
            "  Downloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alive-progress (from pymoo)\n",
            "  Downloading alive_progress-3.1.4-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from pymoo)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pymoo)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.4->pymoo) (0.18.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (2.8.2)\n",
            "Collecting about-time==4.2.1 (from alive-progress->pymoo)\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Collecting grapheme==0.6.0 (from alive-progress->pymoo)\n",
            "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->pymoo) (1.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo) (1.16.0)\n",
            "Building wheels for collected packages: grapheme\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210079 sha256=a7c81c2e40f7eab7dd92ae42af41908397cd8f12d22321503496135a14997bc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e1/49/37e6bde9886439057450c494a79b0bef8bbe897a54aebfc757\n",
            "Successfully built grapheme\n",
            "Installing collected packages: grapheme, dill, Deprecated, cma, about-time, alive-progress, pymoo\n",
            "Successfully installed Deprecated-1.2.14 about-time-4.2.1 alive-progress-3.1.4 cma-3.2.2 dill-0.3.7 grapheme-0.6.0 pymoo-0.6.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pymoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jaFweg5d_Nai"
      },
      "outputs": [],
      "source": [
        "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
        "from pymoo.factory import get_problem, get_reference_directions, get_sampling, get_crossover, get_mutation, get_termination\n",
        "from pymoo.operators.selection.rnd import RandomSelection\n",
        "from pymoo.operators.crossover.sbx import SBX\n",
        "from pymoo.operators.mutation.pm import PolynomialMutation\n",
        "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.optimize import minimize\n",
        "from pymoo.operators.sampling.lhs import LHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JNElZIJdAwqk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J3Tw5cORA7yx"
      },
      "outputs": [],
      "source": [
        "def test_params(**params):\n",
        "    model = RandomForestClassifier(random_state=42, n_jobs=-1, **params).fit(titanic_X_Train, titanic_Y_Train)\n",
        "    train_accuracy_score = accuracy_score(titanic_Y_Train, model.predict(titanic_X_Train))\n",
        "    val_accuracy_score = accuracy_score(titanic_Y_Test, model.predict(titanic_X_Test))\n",
        "    return train_accuracy_score, val_accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "u_sqyoAA_Y1Z"
      },
      "outputs": [],
      "source": [
        "# define the hyperparameter optimization problem\n",
        "class HyperparameterOptimizationProblem(Problem):\n",
        "\n",
        "    def __init__(self,level):\n",
        "        # define the lower and upper bounds of the hyperparameters\n",
        "        # n_estimators: number of trees in the forest (integer)\n",
        "        # max_depth: maximum depth of each tree (integer)\n",
        "        # max_features: maximum number of features (integer)\n",
        "        # min_samples_leaf: minimum number of samples required to be at a leaf node (integer)\n",
        "        self.level = level\n",
        "        self.var_ranges = [\n",
        "            [(10, 500), (2, 8), (2,30), (1,5), (0, 0.3)],\n",
        "            [(10, 600), (2, 12), (2,40), (1,9), (0,0.5)]\n",
        "        ]\n",
        "        xl = np.array([10, 2, 2, 1, 0])\n",
        "        xu = np.array([600, 12, 40, 9, 0.5])\n",
        "\n",
        "        # initialize the problem with 4 variables and 2 objectives\n",
        "        super().__init__(n_var = 5, n_obj = 3,\n",
        "                         xl=[rng[0] for rng in self.var_ranges[level]],\n",
        "                         xu=[rng[1] for rng in self.var_ranges[level]]\n",
        "            )\n",
        "\n",
        "    def _evaluate(self, x, out, *args, **kwargs):\n",
        "        # evaluate each solution (each row of x)\n",
        "        f = np.zeros((x.shape[0], self.n_obj))\n",
        "        for i in range(x.shape[0]):\n",
        "            # get the hyperparameters\n",
        "            n_estimators = int(x[i, 0])\n",
        "            max_depth = int(x[i, 1])\n",
        "            min_samples_split = int(x[i, 2])\n",
        "            min_samples_leaf = int(x[i, 3])\n",
        "            min_weight_fraction_leaf = int(x[i, 4])\n",
        "\n",
        "\n",
        "            # build and train the random forest model\n",
        "            model = RandomForestClassifier(n_estimators=n_estimators,\n",
        "                                           max_depth=max_depth,\n",
        "                                           min_samples_split=min_samples_split,\n",
        "                                           min_samples_leaf = min_samples_leaf,\n",
        "                                           min_weight_fraction_leaf = min_weight_fraction_leaf,\n",
        "                                           max_features=\"sqrt\",\n",
        "                                           random_state=42,\n",
        "                                           n_jobs = -1)\n",
        "            model.fit(titanic_X_Train, titanic_Y_Train)\n",
        "\n",
        "            # predict on the test set\n",
        "            y_pred = model.predict(titanic_X_Test)\n",
        "\n",
        "            # calculate the accuracy, f1 and ROC/AUC score as the objectives\n",
        "            f[i, 0] = -accuracy_score(titanic_Y_Test, y_pred) # negate because we want to maximize\n",
        "            f[i, 1] = -f1_score(titanic_Y_Test, y_pred) # negate because we want to maximize\n",
        "            f[i, 2] = -roc_auc_score(titanic_Y_Test, y_pred) # negate because we want to maximize\n",
        "\n",
        "\n",
        "        # assign the objectives to the output dictionary\n",
        "        out[\"F\"] = f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2UtYt5uI_3aE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a192113-c502-4705-dce7-8ec0850b2351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:8: DeprecationWarning: Call to deprecated function (or staticmethod) get_reference_directions. (Please use `from pymoo.util.ref_dirs import get_reference_directions`)\n",
            "<timed exec>:19: DeprecationWarning: Call to deprecated function (or staticmethod) get_reference_directions. (Please use `from pymoo.util.ref_dirs import get_reference_directions`)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: pop_size=50 is less than the number of reference directions ref_dirs=91.\n",
            "This might cause unwanted behavior of the algorithm. \n",
            "Please make sure pop_size is equal or larger than the number of reference directions. \n",
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |      100 |      1 |             - |             -\n",
            "     2 |      200 |      1 |  0.000000E+00 |             f\n",
            "     3 |      300 |      1 |  0.000000E+00 |             f\n",
            "     4 |      400 |      1 |  0.000000E+00 |             f\n",
            "     5 |      500 |      1 |  0.000000E+00 |             f\n",
            "     6 |      600 |      2 |  1.1618619413 |         ideal\n",
            "     7 |      700 |      2 |  0.000000E+00 |             f\n",
            "     8 |      800 |      2 |  1.0000000000 |         ideal\n",
            "     9 |      900 |      3 |  0.7037037037 |         ideal\n",
            "    10 |     1000 |      3 |  0.000000E+00 |             f\n",
            "    11 |     1100 |      3 |  0.000000E+00 |             f\n",
            "    12 |     1200 |      3 |  0.000000E+00 |             f\n",
            "    13 |     1300 |      3 |  0.000000E+00 |             f\n",
            "    14 |     1400 |      3 |  0.000000E+00 |             f\n",
            "    15 |     1500 |      3 |  0.000000E+00 |             f\n",
            "    16 |     1600 |      3 |  0.000000E+00 |             f\n",
            "    17 |     1700 |      3 |  0.000000E+00 |             f\n",
            "    18 |     1800 |      3 |  0.000000E+00 |             f\n",
            "    19 |     1900 |      3 |  0.000000E+00 |             f\n",
            "    20 |     2000 |      3 |  0.000000E+00 |             f\n",
            "    21 |     2100 |      3 |  0.000000E+00 |             f\n",
            "    22 |     2200 |      3 |  0.000000E+00 |             f\n",
            "    23 |     2300 |      3 |  0.000000E+00 |             f\n",
            "    24 |     2400 |      3 |  0.000000E+00 |             f\n",
            "    25 |     2500 |      3 |  0.000000E+00 |             f\n",
            "    26 |     2600 |      3 |  0.000000E+00 |             f\n",
            "    27 |     2700 |      3 |  0.000000E+00 |             f\n",
            "    28 |     2800 |      3 |  0.000000E+00 |             f\n",
            "    29 |     2900 |      3 |  0.000000E+00 |             f\n",
            "    30 |     3000 |      3 |  0.000000E+00 |             f\n",
            "    31 |     3100 |      3 |  0.000000E+00 |             f\n",
            "    32 |     3200 |      3 |  0.000000E+00 |             f\n",
            "    33 |     3300 |      3 |  0.000000E+00 |             f\n",
            "    34 |     3400 |      3 |  0.000000E+00 |             f\n",
            "    35 |     3500 |      3 |  0.000000E+00 |             f\n",
            "    36 |     3600 |      3 |  0.000000E+00 |             f\n",
            "    37 |     3700 |      3 |  0.000000E+00 |             f\n",
            "    38 |     3800 |      3 |  0.000000E+00 |             f\n",
            "    39 |     3900 |      3 |  0.000000E+00 |             f\n",
            "    40 |     4000 |      3 |  0.000000E+00 |             f\n",
            "    41 |     4100 |      3 |  0.000000E+00 |             f\n",
            "    42 |     4200 |      3 |  0.000000E+00 |             f\n",
            "    43 |     4300 |      3 |  0.000000E+00 |             f\n",
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      1 |             - |             -\n",
            "     2 |      100 |      1 |  0.000000E+00 |             f\n",
            "     3 |      150 |      1 |  0.000000E+00 |             f\n",
            "     4 |      200 |      2 |  3.4210526316 |         ideal\n",
            "     5 |      250 |      3 |  0.7237113402 |         ideal\n",
            "     6 |      300 |      2 |  1.0428015564 |         ideal\n",
            "     7 |      350 |      2 |  0.2180094787 |         nadir\n",
            "     8 |      400 |      1 |  0.0037313433 |         ideal\n",
            "     9 |      450 |      2 |  1.0000000000 |         ideal\n",
            "    10 |      500 |      1 |  0.0037313433 |         ideal\n",
            "    11 |      550 |      1 |  0.000000E+00 |             f\n",
            "    12 |      600 |      1 |  0.000000E+00 |             f\n",
            "    13 |      650 |      1 |  0.000000E+00 |             f\n",
            "    14 |      700 |      1 |  0.000000E+00 |             f\n",
            "    15 |      750 |      1 |  0.000000E+00 |             f\n",
            "    16 |      800 |      1 |  0.000000E+00 |             f\n",
            "    17 |      850 |      1 |  0.000000E+00 |             f\n",
            "    18 |      900 |      1 |  0.000000E+00 |             f\n",
            "    19 |      950 |      1 |  0.000000E+00 |             f\n",
            "    20 |     1000 |      1 |  0.000000E+00 |             f\n",
            "    21 |     1050 |      1 |  0.000000E+00 |             f\n",
            "    22 |     1100 |      2 |  1.0000000000 |         ideal\n",
            "    23 |     1150 |      1 |  0.0069145579 |         ideal\n",
            "    24 |     1200 |      1 |  0.000000E+00 |             f\n",
            "    25 |     1250 |      1 |  0.000000E+00 |             f\n",
            "    26 |     1300 |      1 |  0.000000E+00 |             f\n",
            "    27 |     1350 |      1 |  0.000000E+00 |             f\n",
            "    28 |     1400 |      1 |  0.000000E+00 |             f\n",
            "    29 |     1450 |      1 |  0.000000E+00 |             f\n",
            "    30 |     1500 |      1 |  0.000000E+00 |             f\n",
            "    31 |     1550 |      1 |  0.000000E+00 |             f\n",
            "    32 |     1600 |      1 |  0.000000E+00 |             f\n",
            "    33 |     1650 |      1 |  0.000000E+00 |             f\n",
            "    34 |     1700 |      1 |  0.000000E+00 |             f\n",
            "    35 |     1750 |      1 |  0.000000E+00 |             f\n",
            "    36 |     1800 |      1 |  0.000000E+00 |             f\n",
            "    37 |     1850 |      1 |  0.000000E+00 |             f\n",
            "    38 |     1900 |      1 |  0.000000E+00 |             f\n",
            "    39 |     1950 |      1 |  0.000000E+00 |             f\n",
            "    40 |     2000 |      1 |  0.000000E+00 |             f\n",
            "    41 |     2050 |      1 |  0.000000E+00 |             f\n",
            "    42 |     2100 |      1 |  0.000000E+00 |             f\n",
            "    43 |     2150 |      1 |  0.000000E+00 |             f\n",
            "    44 |     2200 |      1 |  0.000000E+00 |             f\n",
            "    45 |     2250 |      1 |  0.000000E+00 |             f\n",
            "    46 |     2300 |      1 |  0.000000E+00 |             f\n",
            "    47 |     2350 |      1 |  0.000000E+00 |             f\n",
            "    48 |     2400 |      1 |  0.000000E+00 |             f\n",
            "    49 |     2450 |      1 |  0.000000E+00 |             f\n",
            "    50 |     2500 |      1 |  0.000000E+00 |             f\n",
            "Best solution found: \n",
            "X = [[35  8  4  1  0]] \n",
            "F = [[0.82835821 0.76767677 0.8073105 ]]\n",
            "CPU times: user 40min 53s, sys: 3min 51s, total: 44min 44s\n",
            "Wall time: 43min 13s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# create an instance of the problem\n",
        "problem = HyperparameterOptimizationProblem(level=0)\n",
        "problem1 = HyperparameterOptimizationProblem(level=1)\n",
        "\n",
        "# create an instance of NSGA-III algorithm\n",
        "algorithm = NSGA3(\n",
        "    pop_size= 100,\n",
        "    ref_dirs=get_reference_directions(\"das-dennis\", 3, n_partitions=12),\n",
        "    # sampling=get_sampling(\"int_random\"),\n",
        "    sampling=LHS(),\n",
        "    selection = RandomSelection(),\n",
        "    # crossover=get_crossover(\"int_sbx\", prob=0.9, eta=15),\n",
        "    crossover = SBX(prob=0.6, prob_var=0.5),\n",
        "    mutation=PolynomialMutation(prob=0.5),\n",
        "    eliminate_duplicates=True)\n",
        "\n",
        "algorithm1 = NSGA3(\n",
        "    pop_size= 50,\n",
        "    ref_dirs=get_reference_directions(\"das-dennis\", 3, n_partitions=12),\n",
        "    # sampling=get_sampling(\"int_random\"),\n",
        "    sampling=LHS(),\n",
        "    selection = RandomSelection(),\n",
        "    # crossover=get_crossover(\"int_sbx\", prob=0.9, eta=15),\n",
        "    crossover = SBX(prob=0.9, prob_var=0.8),\n",
        "    mutation=PolynomialMutation(prob=0.8),\n",
        "    eliminate_duplicates=True)\n",
        "\n",
        "# create an instance of termination criterion\n",
        "# termination = get_termination(\"n_gen\", 50)\n",
        "\n",
        "# early stop\n",
        "termination = DefaultMultiObjectiveTermination(\n",
        "    xtol=1e-8,           # movement in the design space xtol\n",
        "    cvtol=1e-6,          # the convergence in the constraint cv_tol\n",
        "    ftol=0.0025,         # objective space f_tol.\n",
        "    period=30,\n",
        "    n_max_gen=50,        # maximum number of generations n_max_gen\n",
        "    n_max_evals=100000   # function evaluations n_max_evals\n",
        ")\n",
        "\n",
        "# perform the optimization\n",
        "res = minimize(problem,\n",
        "               algorithm,\n",
        "               termination,\n",
        "               seed=42,\n",
        "               save_history=True,\n",
        "               verbose=True)\n",
        "res1 = minimize(problem1,\n",
        "               algorithm1,\n",
        "               termination,\n",
        "               seed=42,\n",
        "               seed_population=res.pop,\n",
        "               save_history=True,\n",
        "               verbose=True)\n",
        "\n",
        "# print the results\n",
        "print(f\"Best solution found: \\nX = {res1.X.astype(int)} \\nF = {-res1.F}\") # negate F because we maximized\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlFRxFda7ICf"
      },
      "execution_count": 20,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}